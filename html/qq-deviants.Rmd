---
title: "How much does data vary?"
output: 
  html_document:
    toc: true
    toc_float: true
    source_code: embed
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(kableExtra))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(stringr))
library(plotly)
library(grid)
#@
options(digits = 4, scipen = 999999)
library(psych)
library(ggplot2)
library(GGally)
library(lubridate)
library(quantreg)
library(forecast)
library(tidyquant)
library(timetk)
library(quantmod)
library(matrixStats)
library(plotly)
library(quadprog)
#
# returns
#
#stocks_env <- new.env()
symbols <- c("TAN", "USO") #c("ABBV", "AMZN", "AXP", "CVX", "EMR", "HEI", "NTRI", "OC", "ORCL", "SENS", "TMO" )  #c("FXG", "PBJ", "UGE") #c("ENE", "REP", "")
price_tbl <- tq_get(symbols) %>% select(date, symbol, price = adjusted)
# long format ("TIDY") price tibble for possible other work
return_tbl <- na.omit(price_tbl) %>% group_by(symbol) %>% tq_transmute(mutate_fun = periodReturn, period = "daily", type = "log", col_rename = "daily_return") %>% mutate(abs_return = abs(daily_return))
r_2 <- return_tbl %>% select(symbol, date, daily_return) %>% spread(symbol, daily_return)
r_2 <- xts(r_2, r_2$date)[-1, ]
storage.mode(r_2) <- "numeric"
r_2 <- r_2[, -1]
#
# loss
#
price_spread <- price_tbl %>% spread(symbol, price)
price_spread <- xts(price_spread, price_spread$date)
storage.mode(price_spread) <- "numeric" #select(FXG, PBJ, UGE) # 3 risk factors (rf)
price_spread <- price_spread[, -1]
price_0 <- as.numeric(tail(price_spread, 1))
shares <- c(100000, 0) #, 50000)
#price_last <- price_etf[length(price_etf$FXG), 3:5] #(FXG, PBJ, UGE) %>% as.vector()
w <- as.numeric(shares * price_0)
return_hist <- r_2
# Fan these across the length and breadth of the risk factor series
weights_rf <- matrix(w, nrow=nrow(return_hist), ncol=ncol(return_hist), byrow=TRUE)
## We need to compute exp(x) - 1 for very small x: expm1 accomplishes this
loss_rf <- -rowSums(expm1(return_hist) * weights_rf)
loss_rf <- loss_rf[loss_rf > 0]
loss_df <- tibble(loss = loss_rf, distribution = rep("historical", each = length(loss_rf)))
#
ES_calc <- function(data, prob){
  threshold <- quantile(data, prob)
  result <- median(data[data > threshold])
}
#
n_sim <- 1000
n_sample <- 100
prob <- 0.95
ES_sim <- replicate(n_sim, ES_calc(sample(loss_rf, n_sample, replace = TRUE), prob))
summary(ES_sim)
#
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

<script>
function showText(y) {
    var x = document.getElementById(y);
    if (x.style.display === "none") {
        x.style.display = "block";
    } else {
        x.style.display = "none";
    }
}
</script>

## Learning outcomes

1. Using various measures of deviation from a tendency, calculate average scales of data

2. Identify the applicability of each deviation measure to answer business questions.

3. Formulate and explanation an interpretation of deviation measure results to answer business questions.

Deviation, distance, variation, scale, redisual, and even error are very similar concepts. They form the backbone of statistics because they measure how far away actual data is from our model, our supposition, our hypothesis, our belief of where we would like to  think the data is. And of course there are lot ways to compute distance and deviation.

We just computed several locations, trends, averages, percentiles, even quartiles with a box! Each of these are our peculiar mental image of where we think the data ought to be, at least most of the time. The rest of the time data is, where data happens to be when we stumble on it. Getting to an understanding of scale and distance and deviation we had to form a belief, calculate an average, build a trend. Now we move on to a more complete story of any particular data stream.


## Rev up the deuce

This is where we will start, again. This time we are going to mix it up a bit. We are back at our car auction in New Jersey. We have heard enough about price and now we want to know why price might vary. One explanation from our far-reaching understanding of economic choice is that we might be buying car quality. One measure of quality is how many miles the previoius owner drove the car. We read odometers to get that data. We now have two sets (vectors) of data. Each observation is a pair of two observations for each car: price and miles.

```{r auction}
price <- c(12500,
           13350,
           14600,
           15750,
           17500
)
miles <- c(43521,
           31002,
           18868,
           12339,
           9997
) 
car_tbl <- tibble(i = 1:length(price), price = price, miles = miles)

car_tbl %>% 
  kable(align = "c") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>% 
  column_spec(2, bold = T, color = "blue", border_right = FALSE) %>% 
  column_spec(3, bold = T, color = "blue", border_right = FALSE)
```

We can set up three ways of thinking about our car data:

1. Price ($Y_i$) variations on their own: $Y_i=\bar{Y}+e_i$

2. Miles ($X_i$) variations on their own: $X_i=\bar{X}+e_i$

3. Price dependency on miles: $Y_i=b_0+b_1X_i+e_i$

To put all of these models together on one graph we can plot each of the 5 points on a scatter plot like so:

```{r scatter-1}
p <- car_tbl %>% ggplot(aes(x = miles, y = price)) +
  geom_point(color = "blue", size = 3)
ggplotly(p)
```

Let's look at the Y-axis, price first and draw a horizontal line to depict the arithmetic average of price ($Y_i$). Then draw error bars from each data point to this line all in the Y-direction.

```{r scatter-1-1}
Y_bar <- mean(car_tbl$price)
p <- car_tbl %>% ggplot(aes(x = miles, y = price)) +
  geom_point(color = "blue", size = 3) +
  geom_hline(yintercept = Y_bar, color = "red", size = 1.5) +
  geom_segment(aes(x = miles, y = price, xend = miles, yend = Y_bar), alpha = 0.3)
ggplotly(p)
```
This so looks like the error bar plot we generated before where now miles seems to act like the observations index. Try to draw a similar plot of the X-axis error bars around the arithmetic mean of miles.

<br>

<button onclick="showText('myDIV24')">show / hide</button>
<div id="myDIV24" style="display:none;">

<br>

```{r scatter-2}
X_bar <- mean(car_tbl$miles)
p <- car_tbl %>% ggplot(aes(x = miles, y = price)) +
  geom_point(color = "blue", size = 3) +
  geom_vline(xintercept = X_bar, color = "red", size = 1.5) +
  geom_segment(aes(x = miles, y = price, xend = X_bar, yend = price), alpha = 0.3)
ggplotly(p)
```

We just had to rotate  our thnking by 90 degrees clockwise, that's all. Hover over the virtical average lilne to see the mean of miles as `X_bar`.

</div>

<br>

Now let's layer one graph on the other. This will get a little dense, but will not so badly illustrate the interaction of price and miles.

```{r scatter-3}
p <- car_tbl %>% ggplot(aes(x = miles, y = price)) +
  geom_point(color = "blue", size = 3) +
  geom_hline(yintercept = Y_bar, color = "red", size = 1.5) +
  geom_segment(aes(x = miles, y = price, xend = miles, yend = Y_bar), alpha = 0.3) +
  geom_vline(xintercept = X_bar, color = "red", size = 1.5) +
  geom_segment(aes(x = miles, y = price, xend = X_bar, yend = price), alpha = 0.3)
ggplotly(p)
```

We not have price and miles in the cross-hairs and begin to see not just the individual variations of price and miles about their respective means, but can begin to visualize the co-variation of price and miles.

To illustrate model 3, price dependency on miles: $Y_i=b_0+b_1X_i+e_i$, let's draw the best line we can through the scatter, one that will minimize squared deviations of price about our odometer-inspired model of price. Let's leave that calculation till later. Willingly we will suspend any disbelief about that calculation. Let's instead believe the numbers to be true. Let's also keep the cross-hairs. Here we go.

```{r scatter-4}
car_fit <- lm(price ~ miles, data = car_tbl)
b_0 <- 17782.4396
b_1 <- - 0.1314
X <- car_tbl$miles
Y_predicted <- b_0 + b_1 * X
p <- car_tbl %>% ggplot( aes(x = miles, y = price)) + 
  geom_point(color = "blue", size = 3) +
  geom_hline(yintercept = Y_bar, color = "red", size = 1.0) +
  geom_segment(aes(x = miles, y = price, xend = miles, yend = Y_bar), alpha = 0.3, linetype = "dashed" ) +
  geom_vline(xintercept = X_bar, color = "red", size = 1.5) +
  geom_segment(aes(x = miles, y = price, xend = X_bar, yend = price), alpha = 0.3, linetype = "dashed") + 
  geom_smooth(se = FALSE, method = "lm") + 
  geom_segment(aes(xend = miles, yend = Y_predicted), color = "darkblue") + 
  scale_color_continuous(low = "black", high = "red") + guides(color = FALSE) + theme_bw()
ggplotly(p)
```

hover over the dark blue error bars that connect data pairs of miles and price to a new average, a downward-sloping straight (linear that is) line. If we were to be told that the average slope is `r b_1`, what would we think the average Y-intercept is?

<br>

<button onclick="showText('myDIV25')">show / hide</button>
<div id="myDIV25" style="display:none;">

<br>

Our new average price, $\bar{Y}$, is an estimate of the average of $Y$ conditional on miles, $X$ and its average, still $\bar{X}$, is

$$
\bar{Y} = b_0 + b_1 \bar{X}
$$
We know that $\bar{Y}=`r Y_bar`$, $\bar{X}=`r X_bar`$, and have just been informed that the slope parameter $b_1=`r b_1`$. So we plug (that is substitute) these numbers into the formula to get this. 

$$
`r Y_bar`=b_0`r b_1`(`r X_bar`)
$$
We now solve for the Y-intercept $b_0$ to find this result.

$$
b_0=`r Y_bar`+`r -b_1`\bar{X}=`r b_0`
$$
Phew!, but not so bad, just one equation in one unknown.

</div>

<br>

We now have jumped into hyper-space (at least 2-space) to expand our consciousness from univariate to bivariate relationships.

Our next stop is to use these and other versions of deviations to get a handle on the scale, range, variation, deviation, and yes, error in the data, at least with respect to our beliefs about the data.  We just expanded our beliefs from univariate to multivariate. When we get to multivariate we will look at how variations relate to one another, in our case bivariate variations in price relating to variations in miles and vice-versa.

## Blinded by the light

### What's a sample? (again)

Let's recall what a sample is. It is a random draw from a larger group of data called a population. The word random derives from the Frankish (Germanic language from a while back) word _rant_ much like our word rant and eventually meaning (by about 1880) simply indiscriminate. 

Our sample of auction prices is whatever we could get our hands on at the time of the sampling, thus a sort of random draw from the population of all auctioned cars. This begins to allow us to look at the error bars we generated as random deviations from the mean. 

Let's also recall that this drawing of the random sample is the first analytical step after identifying a business question and a population from which to sample.

So we have a sample of two univariate data series. We have also thought that is is reasonable to relate the two series together. Let's now find some measures of scale, deviation, and variation for each of the univariate series.

### 1. Standard deviation

If $x$ is a sample (subset) indexed by $i = 1 \dots N$ with $N$ elements from the population, then we already know that

$$
\bar{X} = \frac{\Sigma_{i=1}^N X_i}{N}
$$

Our model of deviations from the mean produced error terms $e_i=X_i-\bar{X}$, which we assigned to the miles univariate data series. If we were to add up these deviations like this

$$
\Sigma_{i=1}^{5}(X_i-\bar{X})
$$

what would we get?

<br>

<button onclick="showText('myDIV26')">show / hide</button>
<div id="myDIV26" style="display:none;">

<br>

ZERO.

</div>

<br>

Right, this aggregation of deviations will always give us zero, by definition.

Again, like we did to find the best average, let's calculate the sum of squared errors, the $SSE$. Because we sampled the data, the average $SSE$ will have to be divided by $N-1=5-1=4$, called, for the moment, the number of degrees of freedom. This will produce an unbiased measure, another topic for another time!


$$
s^2 = \frac{\Sigma_{i=1}^N (x_i - \bar{x})^2}{N-1}
$$
The squared measure $s^2$ is officially called the variance. It's square root

$$
s = \sqrt{s^2}
$$

is the standard deviation. THe notion of standard is that of an average. We already know what a deviation is.

Let's build a table of four columns to calculate this measure of scale, deviation, error, and variation.

<br>

<button onclick="showText('myDIV27')">show / hide</button>
<div id="myDIV27" style="display:none;">

<br>

```{r miles-table-1}
miles_bar <- mean(miles)
miles_tbl <- tibble(i = 1:length(miles), miles = miles, deviation = miles - miles_bar, `deviation squared` =  (miles - miles_bar)^2)
N <- length(miles_tbl$miles)
deviation_sum <- sum(miles_tbl$deviation)
SSE <- sum(miles_tbl$`deviation squared`)
variance <- SSE/(length(miles_tbl$miles) - 1)
standard_deviation <- sqrt(variance)
miles_tbl %>% 
  kable(align = "c") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>% 
  column_spec(2, bold = T, color = "blue", border_right = FALSE) %>% 
  column_spec(3, bold = T, color = "red", border_right = FALSE) %>% 
  column_spec(4, bold = T, color = "red", border_right = FALSE)
```

Some really big numbers emerge. That is typical and we often try to scale these down when performing computations, again a topic for later.

Let's sum up the deviations again (just to prove they add up to zero) and the squared deviations ($SSE$)

$$
var(miles)=s^2=\frac{`r SSE`}{`r N` - 1}=`r variance`
$$
and then

$$
s=\sqrt{variance}=\sqrt{`r variance`}=`r standard_deviation`
$$

</div>

<br>

What about the standard deviation of price?

<br>

<button onclick="showText('myDIV28')">show / hide</button>
<div id="myDIV28" style="display:none;">

<br>

We should have gotten `r sd(car_tbl$price)`

</div>

<br>

### 2. Robust measures

- **Range**: the distance between the max and min

$$
Range = max(x) - min(x)
$$

- **Interquartile Range, IQR**: $Q_3$ net of $Q_1$ ($P_{75} - P_{25}$) gives us a robust view like that in Tukey's (1977) box plot. What is the IQR of miles and price?

<br>

<button onclick="showText('myDIV29')">show / hide</button>
<div id="myDIV29" style="display:none;">

<br>

We should have gotten 

$$
IQR_{miles}=`r quantile(miles, 0.75)`-`r quantile(miles, 0.25)`=`r quantile(miles, 0.75)-quantile(miles, 0.25)`
$$
and

$$
IQR_{price}=`r quantile(price, 0.75)`-`r quantile(price, 0.25)`=`r quantile(price, 0.75)-quantile(price, 0.25)`
$$

Wider, broader scale than the standard deviations? This measure is robust to highly skewed distributions with thick tails.

</div>

<br>

- **Mean Absolute Deviation**: MAD is robust to outliers.

$$
MAD = \frac{\Sigma_{i=1}^N |X_i - \mu|}{N}
$$
Let's compute this statistic for price and miles.

<br>

<button onclick="showText('myDIV30')">show / hide</button>
<div id="myDIV30" style="display:none;">

<br>

This time we should have gotten for $X_i= miles_i$:

$$
MAD_{miles} = \frac{\Sigma_{i=1}^N |X_i - \mu|}{N} = \frac{`r sum(abs(miles - mean(miles)))`}{`r length(miles)`} = `r sum(abs(miles - mean(miles))) / length(miles)`
$$

</div>

<br>

### 3. Correlation

Correlation measures the degree of relationship between two variables. The measure ranges from a low of -1 to a high of +1. The -1 is a perfectly correlated inverse relationship between two variables. The +1 correlation measure a perfectly possitive relationship between two variables. A 0 indicates no relationship seems to exist. This is not cause and effect, just two variables happening to bump together or not in the street one day in the Bronx. We otherwise call this an antecedent-consequent relationship.

#### Three steps to a correlation.

1. Calculate the covariance between two variables, $X_i$ and $Y_i$. 

$$
cov(X, Y) = s_{xy} = \frac{\Sigma_{i=1}^N(X_i-\bar{X})(Y_i-\bar{Y})}{N-1}
$$
The numerative sums up the pairwaise ups and downs of how $X$ varies with $Y$. This number may net out to positive, negative, or just very close to zero. We lose at least one degree of freedom because we have to use the arithmetic mean to calculate deviations, just like we did for the sample standard deviation.

2. Calculate the standard deviations for each of the two variables, $X_i$ and $Y_i$. First, the variance, here illustrated for $X$, miles in our example.

$$
var(X) = s_{x}^2 = \frac{\Sigma_{i=1}^N(X_i-\bar{X})^2}{N-1} = \frac{`r sum((miles-mean(miles))^2)`}{`r length(miles)`} = `r var(miles)`
$$
$$
s_X = \sqrt{var(X)} = `r sd(miles)` 
$$
and for $Y$ as price we have

$$
s_Y = `r sd(price)`
$$

3. Calculate the ratio of covariance to the product of the standard deviations. This step transform the unwieldly, and hard to interpret covariance from the $-\infty...+\infty$ to the $-1...+1$ range. For samples we call the correlation $r_{XY}$.

$$
r_{XY} = \frac{cov(X,Y)}{s_x s_y} = \frac{`r cov(miles, price)`}{(`r sd(miles)`)(`r sd(price)`) } = `r cor(miles, price)`
$$

Let's build a table and calculate away.

<br>

<button onclick="showText('myDIV31')">show / hide</button>
<div id="myDIV31" style="display:none;">

<br>

First a table.

```{r corr-table}
corr_tbl <- tibble(i = 1:length(miles), `X=miles` = miles, `Y=price` = price, `(X-X_bar)` = miles - mean(miles), `(Y-Y_bar)` = price - mean(price), `(X-X_bar)(Y-Y_bar)` = (miles - mean(miles))*(price - mean(price)))
corr_tbl %>% 
  kable(align = "c") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>% 
  column_spec(1, bold = T, color = "blue", border_right = FALSE)
```

Summing up the cross deviation terms $(X_i-\bar{X})(Y_i-\bar{Y})$ we get

$$
\Sigma_{i=1}^5 (X_i-\bar{X})(Y_i-\bar{Y}) = `r sum(corr_tbl[,6])`
$$
Dividing by the degrees of freedom $N-1=5-1=4$ we then have the sample covariance

$$
cov(X, Y) = \frac{\Sigma_{i=1}^5 (X_i-\bar{X})(Y_i-\bar{Y})}{N-1} = `r sum(corr_tbl[,6])/4`
$$

We already have the standard deviations of miles, `r sd(miles)`, and price, `r sd(price)`, so now we can compute the sample correlation as

$$
r_{XY} = r_{XY} = \frac{cov(X,Y)}{s_x s_y} = `r cor(miles, price)`
$$

</div>

<br>

#### What about the slope?

We will show later (yes, with calculus -- free of charge) that the slope parameter $b_1$ is just the ratio of the covariance of miles ($X$) and price ($Y$) to the variance of miles ($X$.

Should we try it?

<br>

<button onclick="showText('myDIV32')">show / hide</button>
<div id="myDIV32" style="display:none;">

<br>

$$
b_1 = \frac{cov(X,Y)}{var(X)} = \frac{`r cov(miles, price)`}{`r var(miles)`} =   `r cov(miles, price)/var(miles)`
$$

Negatively sloped, reflecting the negative correlation: when variations in miles are positive, variations in prices are negative, on average in this sample.

</div>

<br>

If we find a car at the auction with an odometer reading of 15010 miles. What price would we expect based on our model?

<br>

<button onclick="showText('myDIV33')">show / hide</button>
<div id="myDIV33" style="display:none;">

<br>

Sure. We found that if we knew $b_1=`r b_1`$, then we could calculate at the cross-hairs of average $X$ and $Y$ that $b_0=`r b_0`$. We substitute this into our model to get

$$
price = b_0 + b_1\,\,miles = `r b_0` + (`r b_1`)(15010) = `r b_0+b_1*15010`
$$

Would we be willing to pay that amount? We might want to look at other factors such as the age and condition of the car, among many other features of our auction in New Jersey.

</div>

<br>


## The tails have it!

To round out our discussion about the shape of data, we ask two more questions:

1. What direction do the tails of distribution tend to go, to the right or the left?

2. How thick are the tails?

The first question gets at the asymmetry in lots of data. An example of this is this data on losses from trading common stock in solar ppwer companies. We gather several days of stock prices, then comppute returns as percentage changes in the daily prices. A loss is whenever the returns are negative. Suppose that the latest price per share is USD `r price_0[1]` and we have 100,000 shares.

```{r skew-losses}
p <- ggplot(loss_df, aes(x = loss_rf, fill = distribution)) +
  geom_histogram()
ggplotly(p)
```

Let's eyeball answers to the questions.

<br>

<button onclick="showText('myDIV52')">show / hide</button>
<div id="myDIV52" style="display:none;">

<br>

1. What direction does the distribution tend to?

This data looks like the skewness is to the right. There is a preponderance of observations in the body to the left of the 100000 mark.

The distirbution therefore looks like it is right- or positively-skewed.

2. How thick tailed is the distribution?

The losses seem to be somewhat frequent in the tails. It looks like it might be tick tailed. In financial terms this means that the volatility (standard deviation, IQR) is itself volatile.

</div>

<br>

The answer to these questions can also come from two more aggregations, both based on deviations and variation of data points from thwir means. Skewness needs a direction so a positive metric will mean deviations can be found in the positive tail on average, while a negative metric will indicate a net average long tail in the opposite diredtion. Here is a metric that does this:

$$
skew = \frac{\frac{\Sigma_{i=1}^N (X_i - m)^3}{N-1}}{s^3}
$$


Just like correlation is dimensionless by scaling the covariance with the product of standard deviations, so the skewness measure looks at the cubed deviations per cubed standard deviation.

- What direction is the price data?

<br>

<button onclick="showText('myDIV53')">show / hide</button>
<div id="myDIV53" style="display:none;">

<br>

Using the formula we construct a table like so.

```{r skew-table}
skew_tbl <- tibble(i = 1:length(price), `Y=price` = price, `(Y-Y_bar)` = price - mean(price), `(Y-Y_bar)^3` = (price - mean(price))^3)
N <- length(price)
SCD <- sum(skew_tbl[,4])
numerator <- SCD / (N-1)
skew_tbl %>% 
  kable(align = "c") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>% 
  column_spec(1, bold = T, color = "blue", border_right = FALSE)
```

We see lots of negative cubed deviations. Let's add them up to get `r SCD`. Divide this by $N-1=$`r length(price)` and we get `r numerator`. Dividing by the cube of the standard deviation we see our result.

$$
skew = \frac{\frac{\Sigma_{i=1}^N (X_i - m)^3}{N-1}}{s^3} = \frac{`r numerator`}{`r sd(price)^3`} = `r numerator / sd(price)^3`
$$

We are sure to check out the units of measurement here. The skew is positive. Price variations on average are _above_ their mean. What about miles?

By the by, the solar loss ditribution does indeed compute a positive skewness. But we could see that with our own eyes as well.

</div>

<br>

So now how thick is the tail? Here we can use a variant of the skew just by squaring the variance term. Kurtosis describes the condition of the thickness of the tail. Just as skewness tells us whether the deviations are on average above or below the mean, so kurtosis tells us how volatile the standard deviation is.

Try this formula out for kurtosis on the price variable.

$$
kurtosis = \frac{\frac{\Sigma_{i=1}^N (X_i - m)^4}{N-1}}{s^4}
$$

- How thick or thin are the tails?

<br>

<button onclick="showText('myDIV54')">show / hide</button>
<div id="myDIV54" style="display:none;">

<br>

Using the formula we construct a table like so.

```{r kurt-table}
kurt_tbl <- tibble(i = 1:length(price), `Y=price` = price, `(Y-Y_bar)` = price - mean(price), `(Y-Y_bar)^4` = (price - mean(price))^4)
N <- length(price)
SQD <- sum(skew_tbl[,4])
numerator <- SQD / (N-1)
kurt_tbl %>% 
  kable(align = "c") %>% 
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>% 
  column_spec(1, bold = T, color = "blue", border_right = FALSE)
```

We see lots of negative cubed deviations. Let's add them up to get `r SQD`. Divide this by $N-1=$`r length(price)` and we get `r numerator`. Dividing by the cube of the standard deviation we see our result.

$$
kurtosis = \frac{\frac{\Sigma_{i=1}^N (X_i - m)^4}{N-1}}{s^4} = \frac{`r numerator`}{`r sd(price)^4`} = `r numerator / sd(price)^4`
$$

THe kurtosis is very small, indeed a very thin tale. This indicates a very stable volatility in prices. We will see later with teh normal distribution that the normal kurtosis is 3.00. Compared to the normal distribution, the price distribution is a very thin tailed distribution. An implication might be that it is a rare occurrence to see a high price in this sample. We must remember thata there are only 5 data points in the first place!

The solar loss distribution has a kurtosis that is only slightly greater than 3. 

We are sure to check out the units of measurement here. The skew is positive. Price variations on average are _above_ their mean.  What about miles?

</div>

<br>

## How can we use all of this?

At least five come to mind. All of these build on a foundation of where (location, tendency) a distribution tends to land.

1. **Empirical Rule**: if we thnk the distribution is symmetrical ("normal") then the proportion of observations will be

```{r}
data <- read.csv("data/empirical-rule.csv")
data %>%
  kable("html", escape = F) %>% 
    kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

2. **Chebychev**: at least 75\% of all values are within ±2σ of the mean regardless of the shape of a distribution lie within $\pm 2 \sigma$ of the mean; for $\mu \pm k \sigma$, $k$ standard deviations and the proportion of observations is

$$
1 - \frac{1}{k^2}
$$

3. **Outlier analysis**: using IQR as a quantile inspired deviation we can build **fences**: Tukey(1977) once recommended 

- We add 1.5 times IQR to Q3 and see if any data exceeded that **fence** _and_

- We subtract 1.5 times IQR from Q1 and see if any data fell short of that **fence**


3.  **Coefficient of Variation**: the ratio of the standard deviation to the mean expressed in percentage and is denoted CV

$$
CV = \frac{\sigma}{\mu} \times 100
$$

4. **Comparisons using the z-score**: converts means into standard deviation units; the number of standard deviations a value from the distribution is above or below the mean of the distribution

$$
z = \frac{x - \mu}{\sigma}
$$


5. **Higher moments.** The third and fourth moments are present in the skewnewss (cubed) and kurtosis (fourth power). Very nice math with a meaningful punch. Skewness will help us understand if deviations are above (positive skewness) or below (negative skewness) the average. Kurtosis will tell us if the standard deviation varies more or less than the normal distribution average of 3.00 (to be dug into later, of course).


## Problems, always problems

1.	Shown below are the top nine leading retailers in the United States in a recent year according to Statista.com. Compute range, IQR, MAD, standard deviation, CV, and invoke the empirical rule and Chebychev's Theorem, along with the z-score for each. Treat this as a sample (then what's the population?).

```{r}
data <- read.csv("data/qq-3-20.csv")
data %>%
  kable("html", escape = F) %>% 
    kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
data <- as.numeric(data[,2])
sum_data <- sum(data)
sum_sq_data <- sum(data^2)
N = length(data)
```

<button onclick="showText('myDIV1')">show / hide</button>
<div id="myDIV1" style="display:none;">

<br>

**Answers (so many)**  

- **Range**: the distance between the max and min

$$
Range = |max(x) - min(x)| = `r max(data)`- `r min(data)` = `r max(data) - min(data)`
$$

- **IQR**: $Q_3$ net of $Q_1$ ($P_{75} - P_{25}$)

$$
IQR = Q_3 - Q_2 = `r quantile(data, .75)` - `r quantile(data, .25)` = `r quantile(data, .75) - quantile(data, .25)`
$$

- **Mean Absolute Deviation**: MAD is robust 

$$
MAD = \frac{\Sigma_{i=1}^N |x_i - \mu|}{N}
$$
$$
\mu = \frac{\Sigma_{i=1}^N x_i}{N}=\frac{`r sum(data)`}{`r length(data)`}
$$
$$
MAD = \frac{`r sum(abs(data - mean(data)))`}{`r length(data)`}
$$
- **Standard Deviation**: first the square of the standard deviation is the variance

$$
\sigma^2 = \frac{\Sigma_{i=1}^N (x_i - \mu)^2}{N}
$$
next, the standard deviation is

$$
\sigma = \sqrt{\sigma^2}
$$
A slightly easier way to compute $\sigma^2$ is

$$
\sigma^2 = \frac{\Sigma_{i=1}^N x_i^2 - N(\bar{x})^2}{N}
$$
$$
\sigma^2 = \frac{`r sum(data^2)` - `r length(data)`(`r mean(data)`)^2}{`r length(data)`} = `r sd(data)^2`
$$
$$
\sigma = `r sd(data)`
$$


- **Empirical Rule**: if we think the distribution is symmetrical ("normal") then the proportion of observations will be

and 

- **Chebychev**: at least 75\% of all values are within ±2σ of the mean regardless of the shape of a distribution lie within $\pm 2 \sigma$ of the mean; for $\mu \pm k \sigma$, $k$ standard deviations and the proportion of observations is

$$
1 - \frac{1}{k^2}
$$

```{r}
data <- read.csv("data/empirical-rule-chebychev-3-20.csv")
data %>%
  kable("html", escape = F) %>% 
    kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

- **Coefficient of Variation**: the ratio of the standard deviation to the mean expressed in percentage and is denoted CV

```{r}
data <- read.csv("data/qq-3-20.csv")
data <- as.numeric(data[,2])
```

$$
CV = \frac{\sigma}{\mu} \times 100 = \frac{`r sd(data)`}{`r mean(data)`} \times 100 = `r sd(data) * 100 / mean(data)`
$$

- **z-score**: converts means into standard deviation units; the number of standard deviations a value from the distribution is above or below the mean of the distribution. For the first data point

$$
z = \frac{x - \mu}{\sigma} = \frac{`r data[1]` - `r mean(data)`}{`r sd(data)`}=\frac{`r data[1] - mean(data)`}{`r sd(data)`}
$$

This observation is `r (data[1] - mean(data)) / sd(data)` standard deviations from the mean `r mean(data)`.

- Sample versus population

If $x$ is a sample (subset) indexed by $i = 1 \dots N$ with $N$ elements from the population, then (the same formula!)

$$
\bar{x} = \frac{\Sigma_{i=1}^N x_i}{N} = \frac{`r sum(data)`}{`r length(data)`} = `r mean(data)`
$$

and 

$$
s^2 = \frac{\Sigma_{i=1}^N (x_i - \bar{x})^2}{N-1} - \frac{`r sum((data - mean(data))^2) `}{`r length(data)` -1} = `r sum((data - mean(data))^2) / (length(data) -1)`
$$
and then

$$
s = \sqrt{s^2} = `r (sum((data - mean(data))^2) / (length(data) -1))^0.5`
$$

</div>

<br>

2. Shown below are the _per diem_ business travel expenses in 11 international cities selected from a study conducted for Business Travel News' 2015 Corporate Travel Index, which is an annual study showing the daily cost of business travel in cities across the Globe. The per diem rates include hotel, car, and food expenses. Use this list to calculate the z scores for Lagos, Riyadh, and Bangkok. Treat the list as a sample.

```{r}
library(dplyr)
data_moments <- function(data){
  library(moments)
  mean.r <- mean(data)
  sd.r <- sd(data)
  range.r <- max(data) - min(data)
  IQR.r <- IQR(data)
  median.r <- median(data)
  cv.r <- sd.r / mean.r
  skewness.r <- skewness(data)
  kurtosis.r <- kurtosis(data)
  sum.r <- sum(data)
  sum.sq.r <- sum(data^2)
  result <- data.frame(mean = mean.r, median = median.r, std_dev = sd.r, Range = range.r, IQR = IQR.r, skewness = skewness.r, kurtosis = kurtosis.r)
  #result <- data.frame(result, table = t(result))
  return(result)
}


data <- read.csv("data/qq-3-26.csv")
data %>%
  kable("html", escape = F) %>% 
    kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
data <- as.numeric(data[,2])
```

<button onclick="showText('myDIV2')">show / hide</button>
<div id="myDIV2" style="display:none;">
**Answer**  

- **Range**: the distance between the max and min

$$
Range = |max(x) - min(x)| = `r max(data)`- `r min(data)` = `r max(data) - min(data)`
$$

- **IQR**: $Q_3$ net of $Q_1$ ($P_{75} - P_{25}$)

$$
IQR = Q_3 - Q_2 = `r quantile(data, .75)` - `r quantile(data, .25)` = `r quantile(data, .75) - quantile(data, .25)`
$$

- **Mean Absolute Deviation**: MAD is robust 

$$
MAD = \frac{\Sigma_{i=1}^N |x_i - \mu|}{N}
$$
$$
\mu = \frac{\Sigma_{i=1}^N x_i}{N}=\frac{`r sum(data)`}{`r length(data)`} = `r mean(data)`
$$
$$
MAD = \frac{`r sum(abs(data - mean(data)))`}{`r length(data)`} = `r sum(abs(data - mean(data))) / length(data)` 
$$
- **Standard Deviation**: first the square of the standard deviation is the variance

$$
\sigma^2 = \frac{\Sigma_{i=1}^N (x_i - \mu)^2}{N}
$$
next, the standard deviation is

$$
\sigma = \sqrt{\sigma^2}
$$
A slightly easier way to compute $\sigma^2$ is

$$
\sigma^2 = \frac{\Sigma_{i=1}^N x_i^2 - N(\bar{x})^2}{N}
$$ 
$$
\sigma^2 = \frac{`r sum(data^2)` - `r length(data)`(`r mean(data)`)^2}{`r length(data)`} = `r sd(data)^2`
$$
$$
\sigma = `r sd(data)`
$$


- **Empirical Rule**: if we think the distribution is symmetrical ("normal") then the proportion of observations will be

and 

- **Chebychev**: at least 75\% of all values are within ±2σ of the mean regardless of the shape of a distribution lie within $\pm 2 \sigma$ of the mean; for $\mu \pm k \sigma$, $k$ standard deviations and the proportion of observations is

$$
1 - \frac{1}{k^2}
$$

```{r}
data <- read.csv("data/empirical-rule-chebychev-3-26.csv")
data %>%
  kable("html", escape = F) %>% 
    kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

- **Coefficient of Variation**: the ratio of the standard deviation to the mean expressed in percentage and is denoted CV

```{r}
data <- read.csv("data/qq-3-26.csv")
data <- as.numeric(data[,2])
```

$$
CV = \frac{\sigma}{\mu} \times 100 = \frac{`r sd(data)`}{`r mean(data)`} \times 100 = `r sd(data) * 100 / mean(data)`
$$

- **z-score**: converts means into standard deviation units; the number of standard deviations a value from the distribution is above or below the mean of the distribution. For the first data point

$$
z = \frac{x - \mu}{\sigma} = \frac{`r data[1]` - `r mean(data)`}{`r sd(data)`}=\frac{`r data[1] - mean(data)`}{`r sd(data)`}
$$

This observation is `r (data[1] - mean(data)) / sd(data)` standard deviations from the mean `r mean(data)`.

- Sample versus population

If $x$ is a sample (subset) indexed by $i = 1 \dots N$ with $N$ elements from the population, then (the same formula!)

$$
\bar{x} = \frac{\Sigma_{i=1}^N x_i}{N} = \frac{`r sum(data)`}{`r length(data)`} = `r mean(data)`
$$

and 

$$
s^2 = \frac{\Sigma_{i=1}^N (x_i - \bar{x})^2}{N-1} - \frac{`r sum((data - mean(data))^2) `}{`r length(data)` -1} = `r sum((data - mean(data))^2) / (length(data) -1)`
$$
and then

$$
s = \sqrt{s^2} = `r (sum((data - mean(data))^2) / (length(data) -1))^0.5`
$$

</div>

<br>

## What have we learned?

So much! More to come -- stay tuned.